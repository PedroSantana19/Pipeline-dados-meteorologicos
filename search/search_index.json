{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bem-vindo! Ol\u00e1! \u00c9 um prazer t\u00ea-lo aqui. Este espa\u00e7o foi criado para compartilhar um pouco sobre mim e alguns dos projetos que desenvolvi ao longo dos meus estudos de engenharia de dados. Aqui, voc\u00ea encontrar\u00e1 uma variedade de trabalhos que demonstram minhas habilidades em coleta, processamento e an\u00e1lise de dados. Sinta-se \u00e0 vontade para explorar cada projeto e conhecer mais sobre as t\u00e9cnicas e ferramentas que utilizo. Se tiver alguma d\u00favida ou quiser discutir qualquer um dos projetos, n\u00e3o hesite em entrar em contato. Obrigado pela visita e espero que aproveite a jornada pelo mundo dos dados! Linkedin","title":"P\u00e1gina inicial"},{"location":"#bem-vindo","text":"Ol\u00e1! \u00c9 um prazer t\u00ea-lo aqui. Este espa\u00e7o foi criado para compartilhar um pouco sobre mim e alguns dos projetos que desenvolvi ao longo dos meus estudos de engenharia de dados. Aqui, voc\u00ea encontrar\u00e1 uma variedade de trabalhos que demonstram minhas habilidades em coleta, processamento e an\u00e1lise de dados. Sinta-se \u00e0 vontade para explorar cada projeto e conhecer mais sobre as t\u00e9cnicas e ferramentas que utilizo. Se tiver alguma d\u00favida ou quiser discutir qualquer um dos projetos, n\u00e3o hesite em entrar em contato. Obrigado pela visita e espero que aproveite a jornada pelo mundo dos dados! Linkedin","title":"Bem-vindo!"},{"location":"Academia/","text":"Viv\u00eancia Acad\u00eamica \"Pouco conhecimento faz com que as pessoas se sintam orgulhosas. Muito conhecimento, que se sintam humildes. \u00c9 assim que as espigas sem gr\u00e3os erguem desdenhosamente a cabe\u00e7a para o c\u00e9u, enquanto que as cheias as baixam para a terra, sua m\u00e3e.\" Leonardo da Vinci MBA - Ci\u00eancia de Dados & Analytics - Escola Superior de Agricultura Luiz de Queiroz - USP Inicio 03/2022 - 10/2023 MBA em DSA feito na USP ESALQ. O programa abordou de conceitos fundamentais como estat\u00edstica b\u00e1sica e limpeza de dados, at\u00e9 t\u00e9cnicas avan\u00e7adas de machine learning . Como trabalho de conclus\u00e3o de curso, entreguei uma monografia abordando o uso de Analytics na preven\u00e7\u00e3o a lavagem de dinheiro e financiamento do terrorismo. Gradua\u00e7\u00e3o - Ci\u00eancia da Computa\u00e7\u00e3o - Universidade Nove de Julho Inicio: 03/2017 - 10/2021 Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7\u00e3o na Universidade 9 de Julho. Al\u00e9m da grade curricular e cursos extras, pude atuar no projeto de inicia\u00e7\u00e3o ci\u00eantifica. Nesse trabalho, entregamos o um jogo digital para deficientes audiovisuais totalmente baseado em sons 3D.","title":"Academia"},{"location":"Academia/#vivencia-academica","text":"\"Pouco conhecimento faz com que as pessoas se sintam orgulhosas. Muito conhecimento, que se sintam humildes. \u00c9 assim que as espigas sem gr\u00e3os erguem desdenhosamente a cabe\u00e7a para o c\u00e9u, enquanto que as cheias as baixam para a terra, sua m\u00e3e.\" Leonardo da Vinci","title":"Viv\u00eancia Acad\u00eamica"},{"location":"Academia/#mba-ciencia-de-dados-analytics-escola-superior-de-agricultura-luiz-de-queiroz-usp","text":"Inicio 03/2022 - 10/2023 MBA em DSA feito na USP ESALQ. O programa abordou de conceitos fundamentais como estat\u00edstica b\u00e1sica e limpeza de dados, at\u00e9 t\u00e9cnicas avan\u00e7adas de machine learning . Como trabalho de conclus\u00e3o de curso, entreguei uma monografia abordando o uso de Analytics na preven\u00e7\u00e3o a lavagem de dinheiro e financiamento do terrorismo.","title":"MBA - Ci\u00eancia de Dados &amp; Analytics - Escola Superior de Agricultura Luiz de Queiroz - USP"},{"location":"Academia/#graduacao-ciencia-da-computacao-universidade-nove-de-julho","text":"Inicio: 03/2017 - 10/2021 Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7\u00e3o na Universidade 9 de Julho. Al\u00e9m da grade curricular e cursos extras, pude atuar no projeto de inicia\u00e7\u00e3o ci\u00eantifica. Nesse trabalho, entregamos o um jogo digital para deficientes audiovisuais totalmente baseado em sons 3D.","title":"Gradua\u00e7\u00e3o - Ci\u00eancia da Computa\u00e7\u00e3o - Universidade Nove de Julho"},{"location":"Contatos/","text":"Contatos Caso tenha alguma d\u00favida, sinta-se livre para me procurar. Essas s\u00e3o algumas das minhas redes sociais: Email: pegroeng23@outlook.com Instagram Linkedin","title":"Contatos"},{"location":"Contatos/#contatos","text":"Caso tenha alguma d\u00favida, sinta-se livre para me procurar. Essas s\u00e3o algumas das minhas redes sociais: Email: pegroeng23@outlook.com Instagram Linkedin","title":"Contatos"},{"location":"Experi%C3%AAncia%20Profissional/","text":"Experi\u00eancia profissional Arquiteto de solu\u00e7\u00f5es Pleno Arquiteto de solu\u00e7\u00f5es pleno no Banco Bradesco SA Inicio: 12/2023 [Emprego atual] Atribui\u00e7\u00f5es: Atuar junto dos times de Renegocia\u00e7\u00e3o de D\u00edvidas, migra\u00e7\u00e3o de qualifica\u00e7\u00f5es de Garantias do legado para FICO e Consignado Privado. Entre as atividades, mapear jornadas atuais, jornadas to be , identificar melhorias poss\u00edveis, polinizar conhecimentos Azure, desenhar evolu\u00e7\u00f5es de sistemas existentes e desenhar arquiteturas para novos projetos. Principais tecnologias utilizadas: Azure, Databricks, Kafka, Mainframe Arquiteto de solu\u00e7\u00f5es Arquiteto de solu\u00e7\u00f5es no Banco Bradesco SA - Inicio: 10/2020 - 12/2023 - Atribui\u00e7\u00f5es: Desenhar arquiteturas de infraestrutura dos projetos e provas de conceito feitas - POCs - no InovaBRA LAB. Tamb\u00e9m fazer documenta\u00e7\u00f5es e benchmarks das solu\u00e7\u00f5es que foram demonstradas. Entre as POCs que participei, destaco: Ferramentas de Cr\u00e9dito; Solu\u00e7\u00f5es de preven\u00e7\u00e3o \u00e0 vazamento de dados; Bancos de dados n\u00e3o relacionais; Estagi\u00e1rio Arquitetura de TI - Provas de conceito InovaBRA LAB Est\u00e1gio no Banco Bradesco SA - Inicio: 10/2018 - 10/2020 - Atribui\u00e7\u00f5es: Participar da confec\u00e7\u00e3o do caderno de requisitos para avaliar as solu\u00e7\u00f5es de mercado e validar o atendimento total, parcial ou n\u00e3o atendimento da solu\u00e7\u00e3o desses itens. Quando a POC finalizava, apresentava resultados para lideran\u00e7as e gestores Certifica\u00e7\u00f5es |AZ-900| Azure Fundamentals| |DP-900| Azure Data Fundamentals| |SC-900| Azure Security, Compliance and Identity Fundamentals| |AZ-104| Azure Administrator Associate| |AZ-305| Azure Solutions Archtecht| |Airflow Fundamentals| Astronomer Airflow Fundamentals| |AWS Cloud Practioner| AWS fundamentals|","title":"Experi\u00eancia Profissional"},{"location":"Experi%C3%AAncia%20Profissional/#experiencia-profissional","text":"","title":"Experi\u00eancia profissional"},{"location":"Experi%C3%AAncia%20Profissional/#arquiteto-de-solucoes-pleno","text":"Arquiteto de solu\u00e7\u00f5es pleno no Banco Bradesco SA Inicio: 12/2023 [Emprego atual] Atribui\u00e7\u00f5es: Atuar junto dos times de Renegocia\u00e7\u00e3o de D\u00edvidas, migra\u00e7\u00e3o de qualifica\u00e7\u00f5es de Garantias do legado para FICO e Consignado Privado. Entre as atividades, mapear jornadas atuais, jornadas to be , identificar melhorias poss\u00edveis, polinizar conhecimentos Azure, desenhar evolu\u00e7\u00f5es de sistemas existentes e desenhar arquiteturas para novos projetos. Principais tecnologias utilizadas: Azure, Databricks, Kafka, Mainframe","title":"Arquiteto de solu\u00e7\u00f5es Pleno"},{"location":"Experi%C3%AAncia%20Profissional/#arquiteto-de-solucoes","text":"Arquiteto de solu\u00e7\u00f5es no Banco Bradesco SA - Inicio: 10/2020 - 12/2023 - Atribui\u00e7\u00f5es: Desenhar arquiteturas de infraestrutura dos projetos e provas de conceito feitas - POCs - no InovaBRA LAB. Tamb\u00e9m fazer documenta\u00e7\u00f5es e benchmarks das solu\u00e7\u00f5es que foram demonstradas. Entre as POCs que participei, destaco: Ferramentas de Cr\u00e9dito; Solu\u00e7\u00f5es de preven\u00e7\u00e3o \u00e0 vazamento de dados; Bancos de dados n\u00e3o relacionais;","title":"Arquiteto de solu\u00e7\u00f5es"},{"location":"Experi%C3%AAncia%20Profissional/#estagiario-arquitetura-de-ti-provas-de-conceito-inovabra-lab","text":"Est\u00e1gio no Banco Bradesco SA - Inicio: 10/2018 - 10/2020 - Atribui\u00e7\u00f5es: Participar da confec\u00e7\u00e3o do caderno de requisitos para avaliar as solu\u00e7\u00f5es de mercado e validar o atendimento total, parcial ou n\u00e3o atendimento da solu\u00e7\u00e3o desses itens. Quando a POC finalizava, apresentava resultados para lideran\u00e7as e gestores","title":"Estagi\u00e1rio Arquitetura de TI - Provas de conceito InovaBRA LAB"},{"location":"Experi%C3%AAncia%20Profissional/#certificacoes","text":"|AZ-900| Azure Fundamentals| |DP-900| Azure Data Fundamentals| |SC-900| Azure Security, Compliance and Identity Fundamentals| |AZ-104| Azure Administrator Associate| |AZ-305| Azure Solutions Archtecht| |Airflow Fundamentals| Astronomer Airflow Fundamentals| |AWS Cloud Practioner| AWS fundamentals|","title":"Certifica\u00e7\u00f5es"},{"location":"Habilidades/","text":"Habilidades Hard Skills Python; SQL; NoSQL (MongoDB, CassandraDB); Azure; Databricks; Airflow; Linux/Windows; HTML/CSS; Soft Skills Comunica\u00e7\u00e3o; Resili\u00eancia; Flexibilidade; Proatividade; Constante aprendizado;","title":"Habilidades"},{"location":"Habilidades/#habilidades","text":"","title":"Habilidades"},{"location":"Habilidades/#hard-skills","text":"Python; SQL; NoSQL (MongoDB, CassandraDB); Azure; Databricks; Airflow; Linux/Windows; HTML/CSS;","title":"Hard Skills"},{"location":"Habilidades/#soft-skills","text":"Comunica\u00e7\u00e3o; Resili\u00eancia; Flexibilidade; Proatividade; Constante aprendizado;","title":"Soft Skills"},{"location":"Pipeline/","text":"Extra\u00e7\u00e3o de dados metereol\u00f3gicos Objetivo do Projeto Fomos contratados por uma ag\u00eancia de turismo de Boston que organiza excurs\u00f5es para turistas vindos de diversos pa\u00edses. Para definir os melhores roteiros para cada dia, \u00e9 importante contar com dados atualizados de previs\u00e3o do tempo. Nosso trabalho ser\u00e1 extrair os dados de uma API e disponibizar um arquivo contendo esses dados de previs\u00f5es dos pr\u00f3ximos 07 dias em uma pasta definida pelo time de neg\u00f3cio. Tecnologias e libs utilizadas Python; Pendulum; DAG; Airflow; API REST; 01 - Importanto as libs necess\u00e1rias from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator from airflow.macros import ds_add from airflow import DAG import pendulum from os.path import join import pandas as pd 02 - Criando a DAG with DAG( \"dados_climaticos2\", start_date=pendulum.datetime(2023, 5, 29, tz=\"UTC\"), schedule_interval='0 0 * * 1', # Uso de CRON Expression. Executar toda segunda feira ) as dag: `tarefa_1 = BashOperator( task_id = 'cria_pasta', bash_command = 'mkdir -p \"/home/pedro/Documents/airflowalura/semana={{data_interval_end.strftime(\"%Y-%m-%d\")}}\" )` 03 Criando a fun\u00e7\u00e3o que recebe a chave, cidade e intervalo das datas def extrai_dados(data_interval_end): city = 'Boston' key = 'C6THQDDDE2FKJUB4RCLYRJUN8' `URL = join(\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/\", f\"{city}/{data_interval_end}/{ds_add(data_interval_end, 7)}?unitGroup=metric&include=days&key={key}&contentType=csv\")` `dados = pd.read_csv(URL)` 04 - Preparando o path para gravar os arquivos file_path = f'/home/pedro/Documents/airflowalura/semana={data_interval_end}/' `dados.to_csv(file_path + 'dados_brutos.csv')` `dados[['datetime', 'tempmin', 'temp', 'tempmax']].to_csv(file_path + 'temperatura.csv')` `dados[['datetime', 'description', 'icon']].to_csv(file_path + 'condicoes.csv')` ## 05 - Acionando o operador Python e definindo a prioridade tarefa_2 = PythonOperator( task_id = 'extrai_dados', python_callable= extrai_dados, op_kwargs = {'data_interval_end': '{{data_interval_end.strftime(\"%Y-%m-%d\")}}'} ) `tarefa_1 >> tarefa_2`","title":"Dados metereol\u00f3gicos"},{"location":"Pipeline/#extracao-de-dados-metereologicos","text":"","title":"Extra\u00e7\u00e3o de dados metereol\u00f3gicos"},{"location":"Pipeline/#objetivo-do-projeto","text":"Fomos contratados por uma ag\u00eancia de turismo de Boston que organiza excurs\u00f5es para turistas vindos de diversos pa\u00edses. Para definir os melhores roteiros para cada dia, \u00e9 importante contar com dados atualizados de previs\u00e3o do tempo. Nosso trabalho ser\u00e1 extrair os dados de uma API e disponibizar um arquivo contendo esses dados de previs\u00f5es dos pr\u00f3ximos 07 dias em uma pasta definida pelo time de neg\u00f3cio.","title":"Objetivo do Projeto"},{"location":"Pipeline/#tecnologias-e-libs-utilizadas","text":"Python; Pendulum; DAG; Airflow; API REST;","title":"Tecnologias e libs utilizadas"},{"location":"Pipeline/#01-importanto-as-libs-necessarias","text":"from airflow.operators.bash_operator import BashOperator from airflow.operators.python_operator import PythonOperator from airflow.macros import ds_add from airflow import DAG import pendulum from os.path import join import pandas as pd","title":"01 - Importanto as libs necess\u00e1rias"},{"location":"Pipeline/#02-criando-a-dag","text":"with DAG( \"dados_climaticos2\", start_date=pendulum.datetime(2023, 5, 29, tz=\"UTC\"), schedule_interval='0 0 * * 1', # Uso de CRON Expression. Executar toda segunda feira ) as dag: `tarefa_1 = BashOperator( task_id = 'cria_pasta', bash_command = 'mkdir -p \"/home/pedro/Documents/airflowalura/semana={{data_interval_end.strftime(\"%Y-%m-%d\")}}\" )`","title":"02 - Criando a DAG"},{"location":"Pipeline/#03-criando-a-funcao-que-recebe-a-chave-cidade-e-intervalo-das-datas","text":"def extrai_dados(data_interval_end): city = 'Boston' key = 'C6THQDDDE2FKJUB4RCLYRJUN8' `URL = join(\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/\", f\"{city}/{data_interval_end}/{ds_add(data_interval_end, 7)}?unitGroup=metric&include=days&key={key}&contentType=csv\")` `dados = pd.read_csv(URL)`","title":"03 Criando a fun\u00e7\u00e3o que recebe a chave, cidade e intervalo das datas"},{"location":"Pipeline/#04-preparando-o-path-para-gravar-os-arquivos","text":"file_path = f'/home/pedro/Documents/airflowalura/semana={data_interval_end}/' `dados.to_csv(file_path + 'dados_brutos.csv')` `dados[['datetime', 'tempmin', 'temp', 'tempmax']].to_csv(file_path + 'temperatura.csv')` `dados[['datetime', 'description', 'icon']].to_csv(file_path + 'condicoes.csv')` ## 05 - Acionando o operador Python e definindo a prioridade tarefa_2 = PythonOperator( task_id = 'extrai_dados', python_callable= extrai_dados, op_kwargs = {'data_interval_end': '{{data_interval_end.strftime(\"%Y-%m-%d\")}}'} ) `tarefa_1 >> tarefa_2`","title":"04 - Preparando o path para gravar os arquivos"},{"location":"Produtos/","text":"ETL dados de Produtos Objetivo do Projeto Fomos contratados para sermos engenheiros de dados de um e-commerce. Disponibilizaremos os dados das vendas de 2020 at\u00e9 2023 para os times de neg\u00f3cio. Esses dados est\u00e3o dispon\u00edveis em uma API. Disponibilizaremos os dados no MongoDB Atlas para os times de Data Science. Ap\u00f3s isso, faremos transforma\u00e7\u00f5es nos dados e disponibilizaremos em uma tabela do MySQL para o time de BI. O time de Data Science precisa acessar os dados brutos da forma como est\u00e3o na API. Para atender essa necessidade, nossa primeira tarefa ser\u00e1 extrair esses dados e armazen\u00e1-los em um banco de dados NoSQL Por outro lado, o time de BI requer dados mais organizados e estruturados. Para atend\u00ea-los, vamos ler os dados brutos, aplicar as transforma\u00e7\u00f5es necess\u00e1rias e disponibilizar esses dados em tabelas de um banco de dados relacional. Tecnologias e libs utilizadas Python; MongoDB Atlas; MySQK; API REST; 01 - Importanto as libs necess\u00e1rias `sudo apt update sudo apt upgrade -y python3 --version python3 -V sudo apt install python3-pip -y sudo apt install python3-venv -y Criando pasta para o projeto mkdir pipeline-python-mongo-mysql cd pipeline-python-mongo-mysql python3 -m venv venv source venv/bin/activate Instalando bibliotecas pip install requests==2.31.0 pip install pymongo==4.4.0 pip install pandas==2.0.3 pip install mysql-connector-python==8.0.33` 02 - Extraindo os dados, inserindo no MongoDB Atlas e salvando em um arquivo. ` from pymongo.mongo_client import MongoClient from pymongo.server_api import ServerApi import requests def connect_mongo(uri): # Criar client e conectar no servidor client = MongoClient(uri, server_api=ServerApi('1')) # Enviar ping para confirmar se a conex\u00e3o foi bem sucedida try: client.admin.command('ping') print(\"Pinged your deployment. You successfully connected to MongoDB!\") except Exception as e: print(f\"An error occurred: {e}\") return client def create_connect_db(client, db_name): db = client[db_name] return db def create_connect_collection(db, col_name): collection = db[col_name] return collection def extract_api_data(url): return requests.get(url).json() def insert_data(col, data): docs = col.insert_many(data) n_docs_inseridos = len(docs.inserted_ids) return n_docs_inseridos if name == \" main \": uri = \"mongodb+srv://pedroeng23:12345@expressnodejs.7kz43zl.mongodb.net/?retryWrites=true&w=majority&appName=expressnodejs\" #url1 = \"https://labdados.com/produtos\" client = connect_mongo(uri) db = create_connect_db(client, \"db_produtos_desafio\") col = create_connect_collection(db, \"produtos\") data = extract_api_data(\"https://labdados.com/produtos\") print(f\"\\nQuantidade de dados extraidos: {len(data)}\") n_docs = insert_data(col, data) print(f\"\\n Quantidades de docs inseridos: {n_docs}\") client.close() ` 03 Transformando os dados e salvando os produtos vendidos a partir de de 2021 ` import pandas as pd from extract_and_save import connect_mongo, create_connect_db, create_connect_collection def visualize_collection(col): for doc in col.find(): print(doc) def rename_column(col, col_name, new_name): col.update_many({}, {'$rename': {f\"{col_name}\": f\"{new_name}\"}}) def select_category(category): query = { \"Categoria de Produto\": f\"{category}\"} lista_categoria = [] for doc in col.find(query): lista_categoria.append(doc) return lista_categoria def make_regex(col, regex): query= {\"Data da Compra\": {\"$regex\": f\"{regex}\"}} lista_regex = [] for doc in col.find(query): lista_regex.append(doc) return lista_regex def create_dataframe(lista): df = pd.DataFrame(lista) return df def format_date(df): df[\"Data da Compra\"] = pd.to_datetime(df[\"Data da Compra\"], format=\"%d/%m/%Y\") df['Data da Compra'] = df['Data da Compra'].dt.strftime('%Y-%m-%d') def save_csv(df, path): df.to_csv(path, index=False) print(f\"\\nO arquivo {path} foi salvo\") if name == \" main \": uri = \"mongodb+srv://pedroeng23:12345@expressnodejs.7kz43zl.mongodb.net/?retryWrites=true&w=majority&appName=expressnodejs\" client = connect_mongo(uri) db = create_connect_db(client, \"db_produtos_desafio\") col = create_connect_collection(db, \"produtos\") # renomeando as colunas de latitude e longitude rename_column(col, \"lat\", \"Latitude\") rename_column(col, \"lon\", \"Longitude\") # salvando os dados da categoria livros lst_livros = select_category(col, \"livros\") df_livros = create_dataframe(lst_livros) format_date(df_livros) save_csv(df_livros, \"../data_teste/tb_livros.csv\") # salvando os dados dos produtos vendidos a partir de 2021 lst_produtos = make_regex(col, \"/202[1-9]\") df_produtos = create_dataframe(lst_produtos) format_date(df_produtos) save_csv(df_produtos, \"../data_teste/tb_produtos.csv\") ` 04 Salvando os dados transformados no MySQL e disponibilizando para time de BI. Executa tamb\u00e9m um teste para garantir que as fun\u00e7\u00f5es est\u00e3o rodando corretamente. ` import mysql.connector import pandas as pd def connect_mysql(host_name, user_name, pw): cnx = mysql.connector.connect( host=host_name, user=user_name, password=pw ) print(\"Conex\u00e3o estabelecida:\", cnx) return cnx def create_cursor(cnx): cursor = cnx.cursor() return cursor def create_database(cursor, db_name): query = f\"CREATE DATABASE IF NOT EXISTS {db_name};\" cursor.execute(query) print(f\"Banco de dados '{db_name}' criado ou j\u00e1 existe.\") def show_databases(cursor): cursor.execute('SHOW DATABASES;') databases = cursor.fetchall() print(\"Bancos de dados dispon\u00edveis:\") for db in databases: print(db) def create_product_table(cursor, db_name, tb_name): query = f\"\"\" CREATE TABLE IF NOT EXISTS {db_name}.{tb_name}( id VARCHAR(100), Produto VARCHAR(100), Categoria_Produto VARCHAR(100), Preco DECIMAL(10,2), Frete DECIMAL(10,2), Data_Compra DATE, Vendedor VARCHAR(100), Local_Compra VARCHAR(100), Avaliacao_Compra INT, Tipo_Pagamento VARCHAR(100), Qntd_Parcelas INT, Latitude DECIMAL(10,2), Longitude DECIMAL(10,2), PRIMARY KEY (id) ); \"\"\" cursor.execute(query) print(f\"Tabela '{tb_name}' criada ou j\u00e1 existe no banco de dados '{db_name}'.\") def show_tables(cursor, db_name): query = f\"USE {db_name};\" cursor.execute(query) cursor.fetchall() # Certifique-se de que todos os resultados anteriores foram lidos cursor.execute(\"SHOW TABLES;\") tables = cursor.fetchall() print(f\"Tabelas no banco de dados '{db_name}':\") for tb in tables: print(tb) def read_csv(path): df = pd.read_csv(path) return df def add_product_data(cnx, cursor, df, db_name, tb_name): lista_dados = [tuple(row) for i, row in df.iterrows()] sql_2021 = f\"INSERT INTO {db_name}.{tb_name} VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\" cursor.executemany(sql_2021, lista_dados) cnx.commit() print(f\"Dados inseridos na tabela '{tb_name}' do banco de dados '{db_name}'.\") def test_database_operations(): host_name = 'localhost' user_name = 'dalsin19' pw = '12345' conexao = connect_mysql(host_name, user_name, pw) cursor = create_cursor(conexao) nome_banco = 'db_produtos_teste' create_database(cursor, nome_banco) show_databases(cursor) nome_tabela = 'tb_livros' create_product_table(cursor, nome_banco, nome_tabela) show_tables(cursor, nome_banco) path = '/home/daln19/pipeline-python-mongo-mysql/data/tabela_livros.csv' df = read_csv(path) add_product_data(conexao, cursor, df, nome_banco, nome_tabela) if name == \" main \": test_database_operations() `","title":"Extra\u00e7\u00e3o Labdados"},{"location":"Produtos/#etl-dados-de-produtos","text":"","title":"ETL dados de Produtos"},{"location":"Produtos/#objetivo-do-projeto","text":"Fomos contratados para sermos engenheiros de dados de um e-commerce. Disponibilizaremos os dados das vendas de 2020 at\u00e9 2023 para os times de neg\u00f3cio. Esses dados est\u00e3o dispon\u00edveis em uma API. Disponibilizaremos os dados no MongoDB Atlas para os times de Data Science. Ap\u00f3s isso, faremos transforma\u00e7\u00f5es nos dados e disponibilizaremos em uma tabela do MySQL para o time de BI. O time de Data Science precisa acessar os dados brutos da forma como est\u00e3o na API. Para atender essa necessidade, nossa primeira tarefa ser\u00e1 extrair esses dados e armazen\u00e1-los em um banco de dados NoSQL Por outro lado, o time de BI requer dados mais organizados e estruturados. Para atend\u00ea-los, vamos ler os dados brutos, aplicar as transforma\u00e7\u00f5es necess\u00e1rias e disponibilizar esses dados em tabelas de um banco de dados relacional.","title":"Objetivo do Projeto"},{"location":"Produtos/#tecnologias-e-libs-utilizadas","text":"Python; MongoDB Atlas; MySQK; API REST;","title":"Tecnologias e libs utilizadas"},{"location":"Produtos/#01-importanto-as-libs-necessarias","text":"`sudo apt update sudo apt upgrade -y python3 --version python3 -V sudo apt install python3-pip -y sudo apt install python3-venv -y","title":"01 - Importanto as libs necess\u00e1rias"},{"location":"Produtos/#criando-pasta-para-o-projeto","text":"mkdir pipeline-python-mongo-mysql cd pipeline-python-mongo-mysql python3 -m venv venv source venv/bin/activate","title":"Criando pasta para o projeto"},{"location":"Produtos/#instalando-bibliotecas","text":"pip install requests==2.31.0 pip install pymongo==4.4.0 pip install pandas==2.0.3 pip install mysql-connector-python==8.0.33`","title":"Instalando bibliotecas"},{"location":"Produtos/#02-extraindo-os-dados-inserindo-no-mongodb-atlas-e-salvando-em-um-arquivo","text":"` from pymongo.mongo_client import MongoClient from pymongo.server_api import ServerApi import requests def connect_mongo(uri): # Criar client e conectar no servidor client = MongoClient(uri, server_api=ServerApi('1')) # Enviar ping para confirmar se a conex\u00e3o foi bem sucedida try: client.admin.command('ping') print(\"Pinged your deployment. You successfully connected to MongoDB!\") except Exception as e: print(f\"An error occurred: {e}\") return client def create_connect_db(client, db_name): db = client[db_name] return db def create_connect_collection(db, col_name): collection = db[col_name] return collection def extract_api_data(url): return requests.get(url).json() def insert_data(col, data): docs = col.insert_many(data) n_docs_inseridos = len(docs.inserted_ids) return n_docs_inseridos if name == \" main \": uri = \"mongodb+srv://pedroeng23:12345@expressnodejs.7kz43zl.mongodb.net/?retryWrites=true&w=majority&appName=expressnodejs\" #url1 = \"https://labdados.com/produtos\" client = connect_mongo(uri) db = create_connect_db(client, \"db_produtos_desafio\") col = create_connect_collection(db, \"produtos\") data = extract_api_data(\"https://labdados.com/produtos\") print(f\"\\nQuantidade de dados extraidos: {len(data)}\") n_docs = insert_data(col, data) print(f\"\\n Quantidades de docs inseridos: {n_docs}\") client.close() `","title":"02 - Extraindo os dados, inserindo no MongoDB Atlas e salvando em um arquivo."},{"location":"Produtos/#03-transformando-os-dados-e-salvando-os-produtos-vendidos-a-partir-de-de-2021","text":"` import pandas as pd from extract_and_save import connect_mongo, create_connect_db, create_connect_collection def visualize_collection(col): for doc in col.find(): print(doc) def rename_column(col, col_name, new_name): col.update_many({}, {'$rename': {f\"{col_name}\": f\"{new_name}\"}}) def select_category(category): query = { \"Categoria de Produto\": f\"{category}\"} lista_categoria = [] for doc in col.find(query): lista_categoria.append(doc) return lista_categoria def make_regex(col, regex): query= {\"Data da Compra\": {\"$regex\": f\"{regex}\"}} lista_regex = [] for doc in col.find(query): lista_regex.append(doc) return lista_regex def create_dataframe(lista): df = pd.DataFrame(lista) return df def format_date(df): df[\"Data da Compra\"] = pd.to_datetime(df[\"Data da Compra\"], format=\"%d/%m/%Y\") df['Data da Compra'] = df['Data da Compra'].dt.strftime('%Y-%m-%d') def save_csv(df, path): df.to_csv(path, index=False) print(f\"\\nO arquivo {path} foi salvo\") if name == \" main \": uri = \"mongodb+srv://pedroeng23:12345@expressnodejs.7kz43zl.mongodb.net/?retryWrites=true&w=majority&appName=expressnodejs\" client = connect_mongo(uri) db = create_connect_db(client, \"db_produtos_desafio\") col = create_connect_collection(db, \"produtos\") # renomeando as colunas de latitude e longitude rename_column(col, \"lat\", \"Latitude\") rename_column(col, \"lon\", \"Longitude\") # salvando os dados da categoria livros lst_livros = select_category(col, \"livros\") df_livros = create_dataframe(lst_livros) format_date(df_livros) save_csv(df_livros, \"../data_teste/tb_livros.csv\") # salvando os dados dos produtos vendidos a partir de 2021 lst_produtos = make_regex(col, \"/202[1-9]\") df_produtos = create_dataframe(lst_produtos) format_date(df_produtos) save_csv(df_produtos, \"../data_teste/tb_produtos.csv\") `","title":"03 Transformando os dados e salvando os produtos vendidos a partir de de 2021"},{"location":"Produtos/#04-salvando-os-dados-transformados-no-mysql-e-disponibilizando-para-time-de-bi-executa-tambem-um-teste-para-garantir-que-as-funcoes-estao-rodando-corretamente","text":"` import mysql.connector import pandas as pd def connect_mysql(host_name, user_name, pw): cnx = mysql.connector.connect( host=host_name, user=user_name, password=pw ) print(\"Conex\u00e3o estabelecida:\", cnx) return cnx def create_cursor(cnx): cursor = cnx.cursor() return cursor def create_database(cursor, db_name): query = f\"CREATE DATABASE IF NOT EXISTS {db_name};\" cursor.execute(query) print(f\"Banco de dados '{db_name}' criado ou j\u00e1 existe.\") def show_databases(cursor): cursor.execute('SHOW DATABASES;') databases = cursor.fetchall() print(\"Bancos de dados dispon\u00edveis:\") for db in databases: print(db) def create_product_table(cursor, db_name, tb_name): query = f\"\"\" CREATE TABLE IF NOT EXISTS {db_name}.{tb_name}( id VARCHAR(100), Produto VARCHAR(100), Categoria_Produto VARCHAR(100), Preco DECIMAL(10,2), Frete DECIMAL(10,2), Data_Compra DATE, Vendedor VARCHAR(100), Local_Compra VARCHAR(100), Avaliacao_Compra INT, Tipo_Pagamento VARCHAR(100), Qntd_Parcelas INT, Latitude DECIMAL(10,2), Longitude DECIMAL(10,2), PRIMARY KEY (id) ); \"\"\" cursor.execute(query) print(f\"Tabela '{tb_name}' criada ou j\u00e1 existe no banco de dados '{db_name}'.\") def show_tables(cursor, db_name): query = f\"USE {db_name};\" cursor.execute(query) cursor.fetchall() # Certifique-se de que todos os resultados anteriores foram lidos cursor.execute(\"SHOW TABLES;\") tables = cursor.fetchall() print(f\"Tabelas no banco de dados '{db_name}':\") for tb in tables: print(tb) def read_csv(path): df = pd.read_csv(path) return df def add_product_data(cnx, cursor, df, db_name, tb_name): lista_dados = [tuple(row) for i, row in df.iterrows()] sql_2021 = f\"INSERT INTO {db_name}.{tb_name} VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\" cursor.executemany(sql_2021, lista_dados) cnx.commit() print(f\"Dados inseridos na tabela '{tb_name}' do banco de dados '{db_name}'.\") def test_database_operations(): host_name = 'localhost' user_name = 'dalsin19' pw = '12345' conexao = connect_mysql(host_name, user_name, pw) cursor = create_cursor(conexao) nome_banco = 'db_produtos_teste' create_database(cursor, nome_banco) show_databases(cursor) nome_tabela = 'tb_livros' create_product_table(cursor, nome_banco, nome_tabela) show_tables(cursor, nome_banco) path = '/home/daln19/pipeline-python-mongo-mysql/data/tabela_livros.csv' df = read_csv(path) add_product_data(conexao, cursor, df, nome_banco, nome_tabela) if name == \" main \": test_database_operations() `","title":"04 Salvando os dados transformados no MySQL e disponibilizando para time de BI. Executa tamb\u00e9m um teste para garantir que as fun\u00e7\u00f5es est\u00e3o rodando corretamente."},{"location":"Projetos/","text":"T\u00edtulo da p\u00e1gina Minilista","title":"T\u00edtulo da p\u00e1gina"},{"location":"Projetos/#titulo-da-pagina","text":"","title":"T\u00edtulo da p\u00e1gina"},{"location":"Projetos/#minilista","text":"","title":"Minilista"},{"location":"Quem%20sou%20eu/","text":"Sobre mim Introdu\u00e7\u00e3o Ol\u00e1! Meu nome \u00e9 Pedro Santana e sou Arquiteto de Solu\u00e7\u00f5es. Tenho 6 anos de experi\u00eancia trabalhando com tecnologia no setor financeiro e sou apaixonado por dados. Minha jornada Minha jornada come\u00e7ou ainda pequeno. Sempre gostei muito de videogames e jogos competitivos. No meu \u00faltimo ano do ensino m\u00e9dio, fiz um curso de ver\u00e3o na USP sobre programa\u00e7\u00e3o em C. Desde ent\u00e3o, n\u00e3o tive d\u00favidas que queria trabalhar com tecnologia. Valores e Miss\u00e3o Sou crist\u00e3o, protestante e reformado. Acredito fortemente que devemos promover o uso da tecnologia como um facilitador do dia a dia do nosso pr\u00f3ximo e um mitigador para perigos. Minha miss\u00e3o \u00e9 entregar solu\u00e7\u00f5es robustas, escal\u00e1veis e que atendam o objetivo do neg\u00f3cio de forma eficiente. Tamb\u00e9m desejo polinizar conhecimento de forma gratuita para que o m\u00e1ximo de pessoas que desejem ingressar na \u00e1rea. Fa\u00e7o isso hoje atrav\u00e9s de posts no Linkedin e em brevo pretendo come\u00e7ar um projeto no YouTube. Pr\u00eamios e realiza\u00e7\u00f5es IT Influencer: 10\u00ba projeto mais inovador do Bradesco em 2022; 1\u00ba Est\u00e1giario do Bradesco a receber a medalha \"X\u00e1 Comigo\"; Hobbies Breve lista de hobbies e curiosidades sobre mim Gosto muito de jogos. Alguns dos meus jogos favoritos incluem: The Legend of Zelda, Super Mario, Call of Duty, FIFA e League of Legends. Sou mochileiro e j\u00e1 tive a oportunidade de conhecer 10 pa\u00edses. Entre eles, Egito, Turquia, Israel, Palestina, Sui\u00e7a e Uruguai. Sou estudante de violino e tenho a meta de fazer uma participa\u00e7\u00e3o especial em uma orquestra; Gosto muito de estudar teologia e filosofia; Pratico muscula\u00e7\u00e3o, corrida, reeduca\u00e7\u00e3o postural e jogo futebol semanalmente; Minhas obras nerds favoritas s\u00e3o Star Wars, Senhor dos An\u00e9is e animes em geral. Dos animes meus favoritos s\u00e3o: Dragon Ball, Baki Hanma e Jujutsu Kaisen Torcedor e sofredor Corinthiano;","title":"Sobre mim"},{"location":"Quem%20sou%20eu/#sobre-mim","text":"","title":"Sobre mim"},{"location":"Quem%20sou%20eu/#introducao","text":"Ol\u00e1! Meu nome \u00e9 Pedro Santana e sou Arquiteto de Solu\u00e7\u00f5es. Tenho 6 anos de experi\u00eancia trabalhando com tecnologia no setor financeiro e sou apaixonado por dados.","title":"Introdu\u00e7\u00e3o"},{"location":"Quem%20sou%20eu/#minha-jornada","text":"Minha jornada come\u00e7ou ainda pequeno. Sempre gostei muito de videogames e jogos competitivos. No meu \u00faltimo ano do ensino m\u00e9dio, fiz um curso de ver\u00e3o na USP sobre programa\u00e7\u00e3o em C. Desde ent\u00e3o, n\u00e3o tive d\u00favidas que queria trabalhar com tecnologia.","title":"Minha jornada"},{"location":"Quem%20sou%20eu/#valores-e-missao","text":"Sou crist\u00e3o, protestante e reformado. Acredito fortemente que devemos promover o uso da tecnologia como um facilitador do dia a dia do nosso pr\u00f3ximo e um mitigador para perigos. Minha miss\u00e3o \u00e9 entregar solu\u00e7\u00f5es robustas, escal\u00e1veis e que atendam o objetivo do neg\u00f3cio de forma eficiente. Tamb\u00e9m desejo polinizar conhecimento de forma gratuita para que o m\u00e1ximo de pessoas que desejem ingressar na \u00e1rea. Fa\u00e7o isso hoje atrav\u00e9s de posts no Linkedin e em brevo pretendo come\u00e7ar um projeto no YouTube.","title":"Valores e Miss\u00e3o"},{"location":"Quem%20sou%20eu/#premios-e-realizacoes","text":"IT Influencer: 10\u00ba projeto mais inovador do Bradesco em 2022; 1\u00ba Est\u00e1giario do Bradesco a receber a medalha \"X\u00e1 Comigo\";","title":"Pr\u00eamios e realiza\u00e7\u00f5es"},{"location":"Quem%20sou%20eu/#hobbies","text":"Breve lista de hobbies e curiosidades sobre mim Gosto muito de jogos. Alguns dos meus jogos favoritos incluem: The Legend of Zelda, Super Mario, Call of Duty, FIFA e League of Legends. Sou mochileiro e j\u00e1 tive a oportunidade de conhecer 10 pa\u00edses. Entre eles, Egito, Turquia, Israel, Palestina, Sui\u00e7a e Uruguai. Sou estudante de violino e tenho a meta de fazer uma participa\u00e7\u00e3o especial em uma orquestra; Gosto muito de estudar teologia e filosofia; Pratico muscula\u00e7\u00e3o, corrida, reeduca\u00e7\u00e3o postural e jogo futebol semanalmente; Minhas obras nerds favoritas s\u00e3o Star Wars, Senhor dos An\u00e9is e animes em geral. Dos animes meus favoritos s\u00e3o: Dragon Ball, Baki Hanma e Jujutsu Kaisen Torcedor e sofredor Corinthiano;","title":"Hobbies"}]}